{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 7: Text Network Analysis and Visualization on Agriculture Dataset\n",
        "\n",
        "This notebook constructs a text corpus from categorical fields in `agriculture_crop_yield.csv` and generates:\n",
        "- Word cloud / tag cloud\n",
        "- Word co-occurrence network with community detection\n",
        "- WordTree (Google Charts)\n",
        "- Exportable corpus for InfraNodus\n",
        "\n",
        "It also summarizes insights from the analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "import itertools\n",
        "import re\n",
        "\n",
        "# Paths\n",
        "DATA_PATH = \"/Users/karthikmac/Downloads/DV_USECASE/task 7/agriculture_crop_yield.csv\"\n",
        "OUTPUT_DIR = \"/Users/karthikmac/Downloads/DV_USECASE/task 7/outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Load\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build per-row text corpus from categorical/nominal fields\n",
        "text_fields = [\n",
        "    \"State\",\"Crop_Type\",\"Season\",\"Climate_Zone\",\"Soil_Type\",\"Irrigation_Type\",\n",
        "    # Discretize some numeric or ordinal fields into tokens as well\n",
        "    \"Pest_Infestation_Level\",\"Disease_Incidence\"\n",
        "]\n",
        "\n",
        "# Helper: normalize tokens\n",
        "stopwords = set(\"\"\"a an the and or of in on for to with by from at as is are was were be been being low medium high summer winter continental subtropical mediterranean\"\"\".split())\n",
        "\n",
        "word_pattern = re.compile(r\"[a-zA-Z][a-zA-Z\\-]+\")\n",
        "\n",
        "def normalize(text: str):\n",
        "    tokens = [t.lower() for t in word_pattern.findall(str(text))]\n",
        "    tokens = [t for t in tokens if t not in stopwords]\n",
        "    return tokens\n",
        "\n",
        "row_texts = []\n",
        "for _, row in df.iterrows():\n",
        "    parts = []\n",
        "    for col in text_fields:\n",
        "        val = row.get(col, \"\")\n",
        "        if pd.isna(val):\n",
        "            continue\n",
        "        parts.append(str(val))\n",
        "    # Add discretized bins for precipitation and temperature\n",
        "    precip = row.get(\"Precipitation_mm\", np.nan)\n",
        "    temp = row.get(\"Temperature_Celsius\", np.nan)\n",
        "    fert = row.get(\"Fertilizer_Usage_kg\", np.nan)\n",
        "    if not pd.isna(precip):\n",
        "        precip_bin = pd.qcut(df[\"Precipitation_mm\"], 3, labels=[\"precip_low\",\"precip_med\",\"precip_high\"]).astype(str)[_]\n",
        "        parts.append(precip_bin)\n",
        "    if not pd.isna(temp):\n",
        "        temp_bin = pd.qcut(df[\"Temperature_Celsius\"], 3, labels=[\"temp_low\",\"temp_med\",\"temp_high\"]).astype(str)[_]\n",
        "        parts.append(temp_bin)\n",
        "    if not pd.isna(fert):\n",
        "        fert_bin = pd.qcut(df[\"Fertilizer_Usage_kg\"], 3, labels=[\"fert_low\",\"fert_med\",\"fert_high\"]).astype(str)[_]\n",
        "        parts.append(fert_bin)\n",
        "\n",
        "    text = \" \".join(parts)\n",
        "    tokens = normalize(text)\n",
        "    row_texts.append(tokens)\n",
        "\n",
        "# Flatten for global counts\n",
        "all_tokens = list(itertools.chain.from_iterable(row_texts))\n",
        "\n",
        "# Save corpus lines (space-joined tokens per row)\n",
        "corpus_lines = [\" \".join(toks) for toks in row_texts]\n",
        "corpus_path = os.path.join(OUTPUT_DIR, \"corpus.txt\")\n",
        "with open(corpus_path, \"w\") as f:\n",
        "    f.write(\"\\n\".join(corpus_lines))\n",
        "\n",
        "len(row_texts), len(all_tokens), corpus_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Word frequencies (tag cloud data)\n",
        "freq = Counter(all_tokens)\n",
        "\n",
        "freq_df = pd.DataFrame(freq.most_common(), columns=[\"token\",\"count\"])\n",
        "freq_csv = os.path.join(OUTPUT_DIR, \"tag_frequencies.csv\")\n",
        "freq_df.to_csv(freq_csv, index=False)\n",
        "freq_df.head(20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Word cloud visualization (static)\n",
        "try:\n",
        "    from wordcloud import WordCloud\n",
        "    import matplotlib.pyplot as plt\n",
        "    text_for_wc = \" \".join(all_tokens)\n",
        "    wc = WordCloud(width=1200, height=600, background_color=\"white\").generate(text_for_wc)\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.imshow(wc, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    wc_path = os.path.join(OUTPUT_DIR, \"wordcloud.png\")\n",
        "    plt.savefig(wc_path, bbox_inches=\"tight\")\n",
        "    wc_path\n",
        "except Exception as e:\n",
        "    print(\"WordCloud not available:\", e)\n",
        "    None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build co-occurrence network\n",
        "# Undirected edges for tokens co-present within a row (sliding window not necessary here due to tokens being categorical)\n",
        "\n",
        "min_token_freq = 3  # prune very rare tokens\n",
        "filtered_tokens_per_row = []\n",
        "valid_tokens = {t for t,c in freq.items() if c >= min_token_freq}\n",
        "for toks in row_texts:\n",
        "    toks = [t for t in toks if t in valid_tokens]\n",
        "    filtered_tokens_per_row.append(sorted(set(toks)))\n",
        "\n",
        "edge_weights = Counter()\n",
        "for toks in filtered_tokens_per_row:\n",
        "    for a, b in itertools.combinations(toks, 2):\n",
        "        if a == b:\n",
        "            continue\n",
        "        if a > b:\n",
        "            a, b = b, a\n",
        "        edge_weights[(a,b)] += 1\n",
        "\n",
        "# Create graph and detect communities\n",
        "import networkx as nx\n",
        "G = nx.Graph()\n",
        "for (a,b), w in edge_weights.items():\n",
        "    G.add_edge(a, b, weight=int(w))\n",
        "\n",
        "# Remove isolates that have no edges\n",
        "isolates = list(nx.isolates(G))\n",
        "G.remove_nodes_from(isolates)\n",
        "\n",
        "# Community detection (Louvain if available, else greedy modularity)\n",
        "community_labels = {}\n",
        "try:\n",
        "    import community as community_louvain  # python-louvain\n",
        "    partition = community_louvain.best_partition(G, weight='weight')\n",
        "    community_labels = partition\n",
        "except Exception:\n",
        "    communities = nx.algorithms.community.greedy_modularity_communities(G, weight='weight')\n",
        "    for idx, comm in enumerate(communities):\n",
        "        for node in comm:\n",
        "            community_labels[node] = idx\n",
        "\n",
        "# Export nodes/edges for Gephi/Cytoscape\n",
        "nodes_export = os.path.join(OUTPUT_DIR, \"network_nodes.csv\")\n",
        "edges_export = os.path.join(OUTPUT_DIR, \"network_edges.csv\")\n",
        "\n",
        "nodes_rows = []\n",
        "for n in G.nodes():\n",
        "    nodes_rows.append({\n",
        "        \"id\": n,\n",
        "        \"label\": n,\n",
        "        \"degree\": int(G.degree(n)),\n",
        "        \"freq\": int(freq[n]),\n",
        "        \"community\": int(community_labels.get(n, -1))\n",
        "    })\n",
        "\n",
        "edges_rows = []\n",
        "for a, b, data in G.edges(data=True):\n",
        "    edges_rows.append({\n",
        "        \"source\": a,\n",
        "        \"target\": b,\n",
        "        \"weight\": int(data.get(\"weight\", 1))\n",
        "    })\n",
        "\n",
        "pd.DataFrame(nodes_rows).to_csv(nodes_export, index=False)\n",
        "pd.DataFrame(edges_rows).to_csv(edges_export, index=False)\n",
        "\n",
        "len(G), G.number_of_edges(), nodes_export, edges_export\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick network visualization (spring layout)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12,8))\n",
        "pos = nx.spring_layout(G, seed=42, k=0.3)\n",
        "\n",
        "# Color by community\n",
        "communities = [community_labels.get(n, -1) for n in G.nodes()]\n",
        "unique_comms = sorted(set(communities))\n",
        "comm_to_color = {c: plt.cm.tab20(i % 20) for i,c in enumerate(unique_comms)}\n",
        "node_colors = [comm_to_color[c] for c in communities]\n",
        "\n",
        "sizes = [300 + 20*freq[n] for n in G.nodes()]\n",
        "\n",
        "nx.draw_networkx_nodes(G, pos, node_size=sizes, node_color=node_colors, alpha=0.9)\n",
        "nx.draw_networkx_edges(G, pos, width=[0.5 + 0.3*data['weight'] for _,_,data in G.edges(data=True)], alpha=0.4)\n",
        "labels = {n: n for n in G.nodes()}\n",
        "nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
        "plt.axis('off')\n",
        "net_img = os.path.join(OUTPUT_DIR, \"network.png\")\n",
        "plt.savefig(net_img, dpi=200, bbox_inches='tight')\n",
        "net_img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# WordTree data preparation (simple phrase expansions)\n",
        "# We'll build parent->children expansions based on bigrams from corpus_lines\n",
        "\n",
        "bigram_counts = Counter()\n",
        "for line in corpus_lines:\n",
        "    toks = line.split()\n",
        "    bigram_counts.update(zip(toks, toks[1:]))\n",
        "\n",
        "# Choose a root term to explore in WordTree (most frequent token)\n",
        "root_token = freq_df.iloc[0]['token'] if not freq_df.empty else None\n",
        "root_token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a basic WordTree HTML using Google Charts\n",
        "from html import escape\n",
        "\n",
        "if root_token:\n",
        "    # Build paths from root for a limited depth\n",
        "    depth_limit = 3\n",
        "    paths = []\n",
        "    for line in corpus_lines:\n",
        "        toks = line.split()\n",
        "        for i, t in enumerate(toks):\n",
        "            if t == root_token:\n",
        "                phrase = [t]\n",
        "                j = i + 1\n",
        "                while j < len(toks) and len(phrase) < depth_limit+1:\n",
        "                    phrase.append(toks[j])\n",
        "                    j += 1\n",
        "                paths.append(\" \".join(phrase))\n",
        "\n",
        "    html_path = os.path.join(OUTPUT_DIR, \"wordtree.html\")\n",
        "    with open(html_path, \"w\") as f:\n",
        "        f.write(\"\"\"\n",
        "<!doctype html>\n",
        "<html>\n",
        "  <head>\n",
        "    <meta charset='utf-8'>\n",
        "    <title>WordTree</title>\n",
        "    <script type='text/javascript' src='https://www.gstatic.com/charts/loader.js'></script>\n",
        "    <script type='text/javascript'>\n",
        "      google.charts.load('current', {packages:['wordtree']});\n",
        "      google.charts.setOnLoadCallback(drawChart);\n",
        "      function drawChart() {\n",
        "        var data = google.visualization.arrayToDataTable([\n",
        "          ['Phrases'],\n",
        "\"\"\"\n",
        "        )\n",
        "        for p in paths:\n",
        "            f.write(\"          ['\" + escape(p) + \"'],\\n\")\n",
        "        f.write(\"\"\"\n",
        "        ]);\n",
        "        var options = {\n",
        "          wordtree: {\n",
        "            format: 'implicit',\n",
        "            word: '\"\"\" + escape(root_token) + \"\"\"'\n",
        "          }\n",
        "        };\n",
        "        var chart = new google.visualization.WordTree(document.getElementById('wordtree'));\n",
        "        chart.draw(data, options);\n",
        "      }\n",
        "    </script>\n",
        "  </head>\n",
        "  <body>\n",
        "    <div id='wordtree' style='width: 100%; height: 700px;'></div>\n",
        "  </body>\n",
        "</html>\n",
        "\"\"\")\n",
        "    html_path\n",
        "else:\n",
        "    None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export for InfraNodus: a plain text file with each line as a document\n",
        "infra_path = os.path.join(OUTPUT_DIR, \"infranodus_corpus.txt\")\n",
        "with open(infra_path, \"w\") as f:\n",
        "    for line in corpus_lines:\n",
        "        f.write(line + \"\\n\")\n",
        "infra_path\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
